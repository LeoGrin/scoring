{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"\"\n",
    "openai.api_key = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = pd.read_json(\"data/frichti_clean_prepared (2).jsonl\", lines=True)\n",
    "\n",
    "true_examples = data[\"completion\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# import gpt2 tokenizer from transformers\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "@functools.lru_cache(maxsize=1000, typed=False)\n",
    "def generate_examples_from_prompts(prompt_prefix, prompt_suffix, prompt_list, n_examples_per_prompt=10, engine=\"text-davinci-003\", max_tokens=250):\n",
    "    \"\"\"\n",
    "    Generate examples from a list of prompts, with openai API\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    for prompt in prompt_list:\n",
    "        prompt = prompt_prefix + prompt + prompt_suffix\n",
    "        response = openai.Completion.create(\n",
    "            engine=engine,\n",
    "            prompt=prompt,\n",
    "            temperature=0.7,\n",
    "            max_tokens=max_tokens,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            n=n_examples_per_prompt,\n",
    "            stop=[\"END\"]\n",
    "        )\n",
    "        examples.extend([choice[\"text\"] for choice in response[\"choices\"]])\n",
    "\n",
    "    # Add the stop token to the end of the examples\n",
    "    examples = [example + \"\\nEND\" for example in examples]\n",
    "    return examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt prefix:  Voici des exemples de newsletter Frichti:\n",
      "Example 1 \n",
      " DU FAIT MAISON EN MOINS DE 10 MIN ?????¬†üò±? \n",
      " Contenu: Le plat le plus rapide du monde fait son retour !\n",
      "\n",
      "Quelle surprise, c‚Äôest d√©j√† pr√™t ! üíõ\n",
      "\n",
      "Des amis qui d√©barquent sans pr√©venir ?\n",
      "Frichti √† la rescousse ! On s‚Äôoccupe de tout.\n",
      "Un gratin dauphinois livr√© en 10min !\n",
      "Quelques instants au four, et c‚Äôest d√©j√† l‚Äôheure de passer √† table !\n",
      "On vous dit bon app√©tit üòã\n",
      "\n",
      "La cuisine, c‚Äôest (super) facile avec Frichti !\n",
      "\n",
      "END\n",
      "Example 2 \n",
      " L‚Äôhistoire commence √† 9h45 dans le Dauphin√©? \n",
      " Contenu: Des abricots cultiv√©s sur les hauteurs de la vall√©e du Rh√¥ne.¬†¬†\n",
      "\n",
      "Sur les hauteurs de la vall√©e du Rh√¥ne, Alain cultive des abricots dans son verger certifi√© Haute Valeur Environnementale. En altitude, ce fruit pousse plus lentement, juste le temps qu‚Äôil lui faut pour d√©velopper tous ses ar√¥mes.\n",
      "\n",
      "END\n",
      "Example 3 \n",
      " Mamma Mia !!! üò±üáÆüáπ? \n",
      " Contenu: La nouvelle carte d‚Äô√©t√© de Pastavino est disponible d√®s maintenant chez Frichti\n",
      "\n",
      "La r√©f√©rence de la cuisine italienne renouvelle sa carte pour l‚Äô√©t√© !\n",
      "\n",
      "Cette ann√©e, Pastavino a d√©cid√© de vous offrir un √©t√© sous le signe de la dolce vita, avec des l√©gumes du soleil gourmands et des produits 100% italiens.\n",
      "Rien n‚Äôest laiss√© au hasard, et il y en a pour tous les go√ªts : de la mozzarella di Bufala super onctueuse, des vraies lasagnes v√©g√©tariennes √† la courgette gratin√©e avec de l‚Äôemmental ET de la mozza‚Ä¶ Mention sp√©ciale au dessert extra-frais : une panna cotta confectionn√©e avec des fraises tout juste cueillies.\n",
      "\n",
      "Tutto va bene !\n",
      "\n",
      "END\n",
      "Example 4 \n",
      " Ce qu‚Äôil se passe √† quelques kilom√®tres de Paris.? \n",
      " Contenu: Des salades toutes fra√Æches cultiv√©es dans le respect de la terre.\n",
      "\n",
      "√Ä quelques kilom√®tres de Paris, Adrien perp√©tue le savoir-faire familial acquis sur plusieurs g√©n√©rations ! Alors chez lui, les salades poussent sur une terre bien limoneuse avec une certification Haute Valeur Environnementale.\n",
      "\n",
      "END\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take 4 examples at random for the prompt prefix, and remove them from the dataset\n",
    "# The examples should be long enough to be representative of the dataset\n",
    "\n",
    "seed = 56\n",
    "prompts = sklearn.utils.resample(true_examples, n_samples=4, random_state=seed)\n",
    "\n",
    "prompt_prefix = \"Voici des exemples de newsletter Frichti:\\n\"\n",
    "for i, prompt in enumerate(prompts):\n",
    "    prompt_prefix += f\"Example {i + 1} \\n\"\n",
    "    prompt_prefix += prompt\n",
    "    prompt_prefix += \"\\n\"\n",
    "\n",
    "print(\"Prompt prefix: \", prompt_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate examples from the prompt prefix\n",
    "prompt_suffix = \"Ecris une nouvelle newsletter Frichti du m√™me style que les exemples:\"\n",
    "prompts = ('',) # tuple to be able to use lru_cache\n",
    "max_tokens = 250\n",
    "generated_examples = generate_examples_from_prompts(prompt_prefix, prompt_suffix, prompts, n_examples_per_prompt=100, max_tokens=max_tokens)\n",
    "for i, example in enumerate(generated_examples):\n",
    "    print(f\"Example {i}: {example}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# Compute the number of tokens in all generated examples\n",
    "n_tokens = [len(tokenizer.encode(example)) for example in generated_examples]\n",
    "print(\"Number of tokens in generated examples: \", n_tokens)\n",
    "\n",
    "# Remove the truncated examples\n",
    "max_tokens = 250\n",
    "print(\"Number of examples before removing truncated examples: \", len(generated_examples))\n",
    "generated_examples = [example for example, n_tokens in zip(generated_examples, n_tokens) if n_tokens < max_tokens - 2]\n",
    "print(\"Number of examples after removing truncated examples: \", len(generated_examples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, example in enumerate(generated_examples):\n",
    "    print(f\"Example {i}: {example}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Newtone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pour l'instant j'enl√®ve le menu de la semaine pour avoir un prompt standard\n",
    "# et les keywords aussi\n",
    "# prompt_prefix = \"\"\"\n",
    "# Etape 1 : √©cris une Newsletter en (bo environ 105 mots sur le sujet suivant: Ecris moi une newsletter pour \"Frichti\", notre startup de livraison de plats pr√©par√©s.\n",
    "# Etape 2 : Ecris un objet du mail au d√©but.\n",
    "\n",
    "# Etape 4 : - un ton tr√®s familier et amical\n",
    "# - Utilise le tutoiement\n",
    "# - Ajoute quelques emojis en lien avec l'aliment mentionn√©\n",
    "# - Signe √† la fin l'Equipe Frichti\n",
    "# \"\"\"\n",
    "# prompt_suffix = \"\"\n",
    "# generated_examples = generate_examples_from_prompts(prompt_prefix, prompt_suffix, prompts, n_examples_per_prompt=100)\n",
    "# for i, example in enumerate(generated_examples):\n",
    "#     print(f\"Example {i}: {example}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed the true and generated examples with the openai api and train a classifier\n",
    "import numpy as np\n",
    "\n",
    "@functools.lru_cache(maxsize=1000, typed=False)\n",
    "def get_embedding(examples, model=\"text-embedding-ada-002\"):\n",
    "   #text = text.replace(\"\\n\", \" \")\n",
    "   embeddings =  openai.Embedding.create(input = examples, model=model)['data']\n",
    "   # Create a numpy array of the embeddings\n",
    "   embeddings = np.array([np.array(embedding[\"embedding\"]) for embedding in embeddings])\n",
    "   return embeddings\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a classifier to distinguish between the true and generated examples\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "@functools.lru_cache(maxsize=1000, typed=False)\n",
    "def score_examples_classifier(true_examples, generated_examples, n_true=None, n_trials=1, show_examples=False):\n",
    "    embeddings_generated = get_embedding(tuple((generated_examples)))\n",
    "    embeddings_true = get_embedding(tuple(true_examples))\n",
    "\n",
    "    res_dic  = {}\n",
    "    clf_list = [LogisticRegression, RandomForestClassifier]\n",
    "    clf_names = [\"LogisticRegression\", \"RandomForestClassifier\"]\n",
    "    for clf, clf_name in zip(clf_list, clf_names):\n",
    "        # Create empty arrays for each classifier and each metric\n",
    "        res_dic[clf_name] = {}\n",
    "        res_dic[clf_name][\"accuracy\"] = np.zeros(n_trials)\n",
    "        res_dic[clf_name][\"precision\"] = np.zeros(n_trials)\n",
    "        res_dic[clf_name][\"recall\"] = np.zeros(n_trials)\n",
    "        res_dic[clf_name][\"f1\"] = np.zeros(n_trials)\n",
    "        res_dic[clf_name][\"min_accuracy\"] = np.zeros(n_trials)\n",
    "        res_dic[clf_name][\"mean_generated_proba\"] = np.zeros(n_trials)\n",
    "        res_dic[clf_name][\"mean_true_proba\"] = np.zeros(n_trials)\n",
    "\n",
    "        \n",
    "\n",
    "    if n_true is None:\n",
    "        n_true = len(embeddings_true)\n",
    "    for i in range(n_trials):\n",
    "        chosen_true_examples_indices = np.random.choice(len(embeddings_true), n_true, replace=False)\n",
    "        embeddings_true_chosen = embeddings_true[chosen_true_examples_indices]\n",
    "\n",
    "        X = np.concatenate([embeddings_generated, embeddings_true_chosen])\n",
    "        y = np.concatenate([np.zeros(len(embeddings_generated)), np.ones(n_true)])\n",
    "\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "        for clf, clf_name in zip(clf_list, clf_names):\n",
    "            clf = clf(random_state=i)\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred = clf.predict(X_test)\n",
    "            res_dic[clf_name][\"accuracy\"][i] = accuracy_score(y_test, y_pred)\n",
    "            res_dic[clf_name][\"precision\"][i] = precision_score(y_test, y_pred)\n",
    "            res_dic[clf_name][\"recall\"][i] = recall_score(y_test, y_pred)\n",
    "            res_dic[clf_name][\"f1\"][i] = f1_score(y_test, y_pred)\n",
    "            res_dic[clf_name][\"min_accuracy\"][i] = max(1 - np.mean(y_test), np.mean(y_test))\n",
    "            y_pred_proba = clf.predict_proba(X_test)\n",
    "            # compute the mean probablity of the generated examples being true\n",
    "            res_dic[clf_name][\"mean_generated_proba\"][i] = np.mean(y_pred_proba[y_test == 0, 1])\n",
    "            # compute the mean probablity of the true examples being true\n",
    "            res_dic[clf_name][\"mean_true_proba\"][i] = np.mean(y_pred_proba[y_test == 1, 1])\n",
    "\n",
    "\n",
    "            if show_examples:\n",
    "                # Sort the generated examples by their probability of being true\n",
    "                y_pred_proba = clf.predict_proba(X_test)\n",
    "                y_pred_proba_true = y_pred_proba[:, 1]\n",
    "                y_pred_proba_true_sorted_indices = np.argsort(y_pred_proba_true)[::-1]\n",
    "                print(f\"--------- Classifier: {clf_name} ---------\")\n",
    "                print(\"Generated examples sorted by probability of being true:\")\n",
    "                print(\"----------------------------------------\")\n",
    "                for j in y_pred_proba_true_sorted_indices:\n",
    "                    if y_test[j] == 0: # generated example\n",
    "                        print(f\"Example: {generated_examples[j]}\")\n",
    "                        print(f\"Probability of being true: {y_pred_proba_true[j]}\")\n",
    "                        print(f\"True label: {y_test[j]}\")\n",
    "                        print(f\"Predicted label: {y_pred[j]}\")\n",
    "                        print(\"\")\n",
    "                        print(\"----------------------------------------\")\n",
    "\n",
    "                # Sort the true examples by their probability of being false\n",
    "                y_pred_proba_false = y_pred_proba[:, 0]\n",
    "                y_pred_proba_false_sorted_indices = np.argsort(y_pred_proba_false)[::-1]\n",
    "                print(\"True examples sorted by probability of being false:\")\n",
    "                print(\"----------------------------------------\")\n",
    "                for j in y_pred_proba_false_sorted_indices:\n",
    "                    if y_test[j] == 1:\n",
    "                        print(f\"Example: {true_examples[j]}\")\n",
    "                        print(f\"Probability of being true: {y_pred_proba_true[j]}\")\n",
    "                        print(f\"True label: {y_test[j]}\")\n",
    "                        print(f\"Predicted label: {y_pred[j]}\")\n",
    "                        print(\"\")\n",
    "                        print(\"----------------------------------------\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    return res_dic\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dic = score_examples_classifier(tuple(true_examples), tuple(generated_examples), n_true=100, n_trials=1, show_examples=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another scoring method: compute the perplexity of the true examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexities = [1, 2, 5, 10, 20, 50, 100, 200, 500, 1000, 2000, 5000, 10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1a183873c8e5a6fcf4fd7a76f8cb9101b831d5daea9afacf0aa9cee7ae151037"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
